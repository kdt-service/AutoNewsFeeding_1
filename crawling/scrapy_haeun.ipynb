{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from NaverNews.items import NavernewsItem \n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "category = {\n",
    "  #정치\n",
    "  '100' : ['264','265','268','266','267', '269'],\n",
    "  #세계\n",
    "  '104' : ['231','232','233','234','322']\n",
    "}\n",
    "\n",
    "category_dict = {\n",
    "    '100' : '정치',\n",
    "    '104' : '세계',\n",
    "\n",
    "    #정치\n",
    "    '264' : '대통령실',\n",
    "    '265' : '국회/정당',\n",
    "    '268' : '북한',\n",
    "    '266' : '행정',\n",
    "    '267' : '국방/외교',\n",
    "    '269' : '정치일반',\n",
    "\n",
    "    #세계\n",
    "    '231' : '아시아/호주',\n",
    "    '232' : '미국/중남미',\n",
    "    '233' : '유럽',\n",
    "    '234' : '중동/아프리카',\n",
    "    '322' : '세계일반'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = \"news\"\n",
    "    allowed_domains = [\"news.naver.com\"]\n",
    "    start_urls = [\"http://news.naver.com/\"]\n",
    "\n",
    "    # Scrapy로 크롤링을 실행시키면 이 함수가 실행됩니다. \n",
    "    # 처음 시작하는 요청 url을 만들어 줌\n",
    "    def start_requests(self):\n",
    "\n",
    "        # 탐색할 날짜 리스트 생성\n",
    "        date_range = pd.date_range(start='20230201', end='20230316')\n",
    "        date_list = date_range.strftime(\"%Y%m%d\").to_list()\n",
    "\n",
    "        for date in date_list:\n",
    "            for main, sub_cats in category.items(): \n",
    "                for sub_cat in sub_cats:\n",
    "                    url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2={sub_cat}&sid1={main}&date={date}&page=1'\n",
    "                    # meta는 다양한 정보를 보관하는 보관함\n",
    "                    # yield는 scheduler에 request를 하나씩 넣어주는 역할\n",
    "                    # response.meta를 통해 request에서 보낸 정보에 접근\n",
    "                    ### yield scrapy.Request(url, callback, meta) ###\n",
    "                    # callback: request를 수행한 후 실행할 함수\n",
    "                    # meta: 추가 정보를 담는 딕셔너리 객체. 이후 callback함수에서 이 정보 활용\n",
    "                    yield scrapy.Request(url, self.url_parse, meta = {'page' : 1, 'urls' : [],  'main' : main, 'sub' : sub_cat})\n",
    "\n",
    "    # 각 뉴스기사의 url 가져오기\n",
    "    def url_parse(self, response):\n",
    "        # css 셀렉터\n",
    "        urls = response.css('#main_content dt.photo a::attr(href)').getall()\n",
    "\n",
    "        # 종료 조건. 끝나는 조건을 만족해 return으로 크롤링 종료\n",
    "        # 중복된 url이 없으면 다음페이지 크롤링, 있다면 크롤링 중지\n",
    "        # 이전 페이지에서 보관한 urls 목록을 가져와서 urls 변수와 비교\n",
    "        # pop() 메서드는 urls키와 해당하는 값을 삭제하면서 반환\n",
    "        if response.meta.pop('urls') == urls :\n",
    "            return \n",
    "        \n",
    "        # 기사 본문을 하나씩 가져오기\n",
    "        for url in urls :\n",
    "            yield scrapy.Request(url, self.parse_news, meta = response.meta)\n",
    "\n",
    "        # 다음 페이지\n",
    "        page = response.meta.pop('page')\n",
    "        # 'page=숫자'일 때 'page=숫자+1'로 대체해 다음페이지 url 만들어줌\n",
    "        # re.sub(pattern, replace text, string, count=0)\n",
    "        next_page_url = re.sub('page\\=\\d+', f'page={page+1}', response.url)\n",
    "        # page가 +1 되고, urls 빈 리스트에 이제는 현재 페이지의 뉴스기사가 리스트로 저장됨\n",
    "        # page 1 증가시킨 사이트(다음페이지)에 요청\n",
    "        yield scrapy.Request(next_page_url, self.url_parse, meta={**response.meta, 'page':page+1, 'urls': urls})\n",
    "\n",
    "\n",
    "    # 뉴스기사 상세 정보 크롤링\n",
    "    def parse_news(self, response):\n",
    "\n",
    "        item = NavernewsItem()\n",
    "        # 기사 제목\n",
    "        item['title'] = response.css('#title_area.media_end_head_headline span::text').get()\n",
    "        # 언론사명\n",
    "        item['source'] = response.css('#ct a img::attr(title)').get()\n",
    "        item['platform'] = '네이버'\n",
    "        # 카테고리\n",
    "        item['main_category'] = category_dict[response.meta.pop('main')]\n",
    "        item['sub_category'] = category_dict[response.meta.pop('sub')]\n",
    "        # 본문 내용\n",
    "        sen_list = response.css('#dic_area::text').getall()\n",
    "        item['content'] = ' '.join(sen_list).strip()\n",
    "        # 작성 날짜\n",
    "        item['writed_at'] = response.css('.media_end_head_info_datestamp_bunch span::attr(data-date-time)').getall()[0]\n",
    "        # 기자 이름\n",
    "        item['writer'] = response.css('.media_end_head_journalist_name::text').get()\n",
    "\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158267"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv('./NaverNews/NaverNews/spiders/NaverNews.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
